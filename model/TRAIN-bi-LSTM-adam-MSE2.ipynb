{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jMkgfO443DF6"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucRIlGFk2_z0",
        "outputId": "38c8222e-31a0-4c6c-ee00-a6abcff0bdca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bobby\\anaconda3\\envs\\research\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "c:\\Users\\bobby\\anaconda3\\envs\\research\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Input, TimeDistributed, RepeatVector, Dense\n",
        "from tensorflow.keras.layers import Bidirectional, \\\n",
        "    multiply, concatenate, Flatten, Activation, dot\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "import keras\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "mae, rmse, r2 = mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# !pip install tensorflow_addons==0.16.1\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W_OeNEX43N-u"
      },
      "outputs": [],
      "source": [
        "def readdata(inputcsv, outputcsv):\n",
        "\n",
        "    idx = pd.IndexSlice\n",
        "    input_data2 = pd.read_csv(inputcsv, index_col=[0], header=[0,1])\n",
        "    input_data2.index = pd.to_datetime(input_data2.index)\n",
        "    input_data2.columns = input_data2.columns.set_levels(input_data2.columns.levels[0].astype('int64'), level=0)\n",
        "    input_data2.columns = input_data2.columns.set_levels(input_data2.columns.levels[1].astype('string'), level=1)\n",
        "\n",
        "    ground_truth2 = pd.read_csv(outputcsv, index_col=[0], header=[0,1])\n",
        "    ground_truth2.index = pd.to_datetime(ground_truth2.index)\n",
        "    ground_truth2.columns = ground_truth2.columns.set_levels(ground_truth2.columns.levels[0].astype('int64'), level=0)\n",
        "    ground_truth2.columns = ground_truth2.columns.set_levels(ground_truth2.columns.levels[1].astype('string'), level=1)\n",
        "\n",
        "    log_transform = lambda x: np.log10(x+1) if x.name[1] == 'tp' else x\n",
        "    input_data2 = input_data2.apply(log_transform)\n",
        "    ground_truth2 = ground_truth2.apply(log_transform)\n",
        "\n",
        "    scaledx = MinMaxScaler()\n",
        "    scaled_input = scaledx.fit_transform(input_data2.values)\n",
        "    scaled_input_df = pd.DataFrame(scaled_input, index=input_data2.index, columns=input_data2.columns)\n",
        "\n",
        "    scaledy = MinMaxScaler()\n",
        "    scaled_ground = scaledy.fit_transform(ground_truth2.values)\n",
        "    scaled_ground_df = pd.DataFrame(scaled_ground, index=ground_truth2.index, columns=ground_truth2.columns)\n",
        "\n",
        "    frames = [scaled_input_df, scaled_ground_df]\n",
        "    dataset = pd.concat(frames, axis=1)\n",
        "\n",
        "    train_dataset = dataset.loc['2000-01-01':'2016-12-31']\n",
        "    test_dataset = dataset.loc['2017-01-01':'2019-12-31']\n",
        "\n",
        "    return train_dataset, test_dataset, scaledx, scaledy\n",
        "\n",
        "post_PATH = 'C:\\\\Users\\\\bobby\\\\Documents\\\\GitHub\\\\attentionMedium\\\\postprocessed_data\\\\'\n",
        "train_dataset, test_dataset, scaledx, scaledy = readdata(post_PATH+'input_data.csv', post_PATH+'ground_truth.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>leadtime</th>\n",
              "      <th colspan=\"5\" halign=\"left\">1</th>\n",
              "      <th colspan=\"5\" halign=\"left\">2</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"4\" halign=\"left\">9</th>\n",
              "      <th colspan=\"5\" halign=\"left\">10</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vars</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>...</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>tp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000-01-01</th>\n",
              "      <td>0.454364</td>\n",
              "      <td>0.414554</td>\n",
              "      <td>0.665413</td>\n",
              "      <td>0.788165</td>\n",
              "      <td>0.382167</td>\n",
              "      <td>0.569784</td>\n",
              "      <td>0.516938</td>\n",
              "      <td>0.683578</td>\n",
              "      <td>0.992458</td>\n",
              "      <td>0.366075</td>\n",
              "      <td>...</td>\n",
              "      <td>0.078876</td>\n",
              "      <td>0.478847</td>\n",
              "      <td>0.362500</td>\n",
              "      <td>0.267941</td>\n",
              "      <td>0.530388</td>\n",
              "      <td>0.412039</td>\n",
              "      <td>0.538813</td>\n",
              "      <td>0.492143</td>\n",
              "      <td>0.360572</td>\n",
              "      <td>0.440304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-02</th>\n",
              "      <td>0.498040</td>\n",
              "      <td>0.394784</td>\n",
              "      <td>0.614060</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>0.349166</td>\n",
              "      <td>0.475599</td>\n",
              "      <td>0.553164</td>\n",
              "      <td>0.636443</td>\n",
              "      <td>0.991829</td>\n",
              "      <td>0.262131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.412755</td>\n",
              "      <td>0.569225</td>\n",
              "      <td>0.657500</td>\n",
              "      <td>0.293947</td>\n",
              "      <td>0.444551</td>\n",
              "      <td>0.521796</td>\n",
              "      <td>0.525433</td>\n",
              "      <td>0.962916</td>\n",
              "      <td>0.246420</td>\n",
              "      <td>0.431659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-03</th>\n",
              "      <td>0.533352</td>\n",
              "      <td>0.219822</td>\n",
              "      <td>0.588761</td>\n",
              "      <td>0.827255</td>\n",
              "      <td>0.323814</td>\n",
              "      <td>0.562780</td>\n",
              "      <td>0.477627</td>\n",
              "      <td>0.639143</td>\n",
              "      <td>0.834067</td>\n",
              "      <td>0.269942</td>\n",
              "      <td>...</td>\n",
              "      <td>0.621661</td>\n",
              "      <td>0.528254</td>\n",
              "      <td>0.868750</td>\n",
              "      <td>0.279250</td>\n",
              "      <td>0.368007</td>\n",
              "      <td>0.435447</td>\n",
              "      <td>0.512694</td>\n",
              "      <td>0.969830</td>\n",
              "      <td>0.254245</td>\n",
              "      <td>0.282224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-04</th>\n",
              "      <td>0.528692</td>\n",
              "      <td>0.439931</td>\n",
              "      <td>0.585007</td>\n",
              "      <td>0.878862</td>\n",
              "      <td>0.310964</td>\n",
              "      <td>0.323332</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.569462</td>\n",
              "      <td>0.981772</td>\n",
              "      <td>0.125560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.530661</td>\n",
              "      <td>0.525183</td>\n",
              "      <td>0.865000</td>\n",
              "      <td>0.227981</td>\n",
              "      <td>0.476429</td>\n",
              "      <td>0.433728</td>\n",
              "      <td>0.545146</td>\n",
              "      <td>0.899434</td>\n",
              "      <td>0.274674</td>\n",
              "      <td>0.474193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000-01-05</th>\n",
              "      <td>0.365374</td>\n",
              "      <td>0.481476</td>\n",
              "      <td>0.559571</td>\n",
              "      <td>0.899403</td>\n",
              "      <td>0.232568</td>\n",
              "      <td>0.126932</td>\n",
              "      <td>0.792039</td>\n",
              "      <td>0.480211</td>\n",
              "      <td>0.998114</td>\n",
              "      <td>0.038795</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361703</td>\n",
              "      <td>0.506658</td>\n",
              "      <td>0.650625</td>\n",
              "      <td>0.274030</td>\n",
              "      <td>0.497078</td>\n",
              "      <td>0.088718</td>\n",
              "      <td>0.371129</td>\n",
              "      <td>0.543055</td>\n",
              "      <td>0.120377</td>\n",
              "      <td>0.442982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-27</th>\n",
              "      <td>0.478379</td>\n",
              "      <td>0.437978</td>\n",
              "      <td>0.575080</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>0.172611</td>\n",
              "      <td>0.224667</td>\n",
              "      <td>0.449791</td>\n",
              "      <td>0.426003</td>\n",
              "      <td>0.964802</td>\n",
              "      <td>0.018779</td>\n",
              "      <td>...</td>\n",
              "      <td>0.374733</td>\n",
              "      <td>0.681752</td>\n",
              "      <td>0.754375</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.608498</td>\n",
              "      <td>0.420226</td>\n",
              "      <td>0.629725</td>\n",
              "      <td>0.920805</td>\n",
              "      <td>0.348392</td>\n",
              "      <td>0.498704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-28</th>\n",
              "      <td>0.442718</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.454412</td>\n",
              "      <td>0.910384</td>\n",
              "      <td>0.081907</td>\n",
              "      <td>0.522516</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479768</td>\n",
              "      <td>0.888121</td>\n",
              "      <td>0.042124</td>\n",
              "      <td>...</td>\n",
              "      <td>0.449595</td>\n",
              "      <td>0.640450</td>\n",
              "      <td>0.924375</td>\n",
              "      <td>0.234016</td>\n",
              "      <td>0.582628</td>\n",
              "      <td>0.413910</td>\n",
              "      <td>0.544096</td>\n",
              "      <td>0.973602</td>\n",
              "      <td>0.162912</td>\n",
              "      <td>0.420299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-29</th>\n",
              "      <td>0.479029</td>\n",
              "      <td>0.061326</td>\n",
              "      <td>0.500099</td>\n",
              "      <td>0.905832</td>\n",
              "      <td>0.138873</td>\n",
              "      <td>0.603320</td>\n",
              "      <td>0.314504</td>\n",
              "      <td>0.634513</td>\n",
              "      <td>0.913262</td>\n",
              "      <td>0.214075</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227360</td>\n",
              "      <td>0.565322</td>\n",
              "      <td>0.998750</td>\n",
              "      <td>0.146430</td>\n",
              "      <td>0.661096</td>\n",
              "      <td>0.340489</td>\n",
              "      <td>0.625849</td>\n",
              "      <td>0.981772</td>\n",
              "      <td>0.278181</td>\n",
              "      <td>0.038538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-30</th>\n",
              "      <td>0.545704</td>\n",
              "      <td>0.372505</td>\n",
              "      <td>0.600421</td>\n",
              "      <td>0.645576</td>\n",
              "      <td>0.225309</td>\n",
              "      <td>0.587668</td>\n",
              "      <td>0.460916</td>\n",
              "      <td>0.680944</td>\n",
              "      <td>0.871779</td>\n",
              "      <td>0.271051</td>\n",
              "      <td>...</td>\n",
              "      <td>0.362376</td>\n",
              "      <td>0.641258</td>\n",
              "      <td>0.996875</td>\n",
              "      <td>0.274139</td>\n",
              "      <td>0.558021</td>\n",
              "      <td>0.474441</td>\n",
              "      <td>0.621887</td>\n",
              "      <td>0.989315</td>\n",
              "      <td>0.222734</td>\n",
              "      <td>0.352531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-31</th>\n",
              "      <td>0.552916</td>\n",
              "      <td>0.522450</td>\n",
              "      <td>0.673749</td>\n",
              "      <td>0.865775</td>\n",
              "      <td>0.298267</td>\n",
              "      <td>0.578814</td>\n",
              "      <td>0.416016</td>\n",
              "      <td>0.721424</td>\n",
              "      <td>0.973602</td>\n",
              "      <td>0.297106</td>\n",
              "      <td>...</td>\n",
              "      <td>0.068481</td>\n",
              "      <td>0.537430</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.248043</td>\n",
              "      <td>0.590371</td>\n",
              "      <td>0.237360</td>\n",
              "      <td>0.629197</td>\n",
              "      <td>0.979887</td>\n",
              "      <td>0.363212</td>\n",
              "      <td>0.484940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6210 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "leadtime          1                                                 2   \\\n",
              "vars             t2m       tp        H         C         E         t2m   \n",
              "2000-01-01  0.454364  0.414554  0.665413  0.788165  0.382167  0.569784   \n",
              "2000-01-02  0.498040  0.394784  0.614060  0.910384  0.349166  0.475599   \n",
              "2000-01-03  0.533352  0.219822  0.588761  0.827255  0.323814  0.562780   \n",
              "2000-01-04  0.528692  0.439931  0.585007  0.878862  0.310964  0.323332   \n",
              "2000-01-05  0.365374  0.481476  0.559571  0.899403  0.232568  0.126932   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2016-12-27  0.478379  0.437978  0.575080  0.910384  0.172611  0.224667   \n",
              "2016-12-28  0.442718  0.296664  0.454412  0.910384  0.081907  0.522516   \n",
              "2016-12-29  0.479029  0.061326  0.500099  0.905832  0.138873  0.603320   \n",
              "2016-12-30  0.545704  0.372505  0.600421  0.645576  0.225309  0.587668   \n",
              "2016-12-31  0.552916  0.522450  0.673749  0.865775  0.298267  0.578814   \n",
              "\n",
              "leadtime                                            ...        9             \\\n",
              "vars             tp        H         C         E    ...       tp        H     \n",
              "2000-01-01  0.516938  0.683578  0.992458  0.366075  ...  0.078876  0.478847   \n",
              "2000-01-02  0.553164  0.636443  0.991829  0.262131  ...  0.412755  0.569225   \n",
              "2000-01-03  0.477627  0.639143  0.834067  0.269942  ...  0.621661  0.528254   \n",
              "2000-01-04  0.490424  0.569462  0.981772  0.125560  ...  0.530661  0.525183   \n",
              "2000-01-05  0.792039  0.480211  0.998114  0.038795  ...  0.361703  0.506658   \n",
              "...              ...       ...       ...       ...  ...       ...       ...   \n",
              "2016-12-27  0.449791  0.426003  0.964802  0.018779  ...  0.374733  0.681752   \n",
              "2016-12-28  0.000000  0.479768  0.888121  0.042124  ...  0.449595  0.640450   \n",
              "2016-12-29  0.314504  0.634513  0.913262  0.214075  ...  0.227360  0.565322   \n",
              "2016-12-30  0.460916  0.680944  0.871779  0.271051  ...  0.362376  0.641258   \n",
              "2016-12-31  0.416016  0.721424  0.973602  0.297106  ...  0.068481  0.537430   \n",
              "\n",
              "leadtime                              10                                \\\n",
              "vars             C         E         t2m       tp        H         C     \n",
              "2000-01-01  0.362500  0.267941  0.530388  0.412039  0.538813  0.492143   \n",
              "2000-01-02  0.657500  0.293947  0.444551  0.521796  0.525433  0.962916   \n",
              "2000-01-03  0.868750  0.279250  0.368007  0.435447  0.512694  0.969830   \n",
              "2000-01-04  0.865000  0.227981  0.476429  0.433728  0.545146  0.899434   \n",
              "2000-01-05  0.650625  0.274030  0.497078  0.088718  0.371129  0.543055   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2016-12-27  0.754375  0.390794  0.608498  0.420226  0.629725  0.920805   \n",
              "2016-12-28  0.924375  0.234016  0.582628  0.413910  0.544096  0.973602   \n",
              "2016-12-29  0.998750  0.146430  0.661096  0.340489  0.625849  0.981772   \n",
              "2016-12-30  0.996875  0.274139  0.558021  0.474441  0.621887  0.989315   \n",
              "2016-12-31  0.820000  0.248043  0.590371  0.237360  0.629197  0.979887   \n",
              "\n",
              "leadtime                    0   \n",
              "vars             E         tp   \n",
              "2000-01-01  0.360572  0.440304  \n",
              "2000-01-02  0.246420  0.431659  \n",
              "2000-01-03  0.254245  0.282224  \n",
              "2000-01-04  0.274674  0.474193  \n",
              "2000-01-05  0.120377  0.442982  \n",
              "...              ...       ...  \n",
              "2016-12-27  0.348392  0.498704  \n",
              "2016-12-28  0.162912  0.420299  \n",
              "2016-12-29  0.278181  0.038538  \n",
              "2016-12-30  0.222734  0.352531  \n",
              "2016-12-31  0.363212  0.484940  \n",
              "\n",
              "[6210 rows x 51 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>leadtime</th>\n",
              "      <th colspan=\"5\" halign=\"left\">1</th>\n",
              "      <th colspan=\"5\" halign=\"left\">2</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"4\" halign=\"left\">9</th>\n",
              "      <th colspan=\"5\" halign=\"left\">10</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vars</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>...</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>t2m</th>\n",
              "      <th>tp</th>\n",
              "      <th>H</th>\n",
              "      <th>C</th>\n",
              "      <th>E</th>\n",
              "      <th>tp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-01</th>\n",
              "      <td>0.606345</td>\n",
              "      <td>0.221450</td>\n",
              "      <td>0.681078</td>\n",
              "      <td>0.880910</td>\n",
              "      <td>0.327974</td>\n",
              "      <td>0.586197</td>\n",
              "      <td>0.621545</td>\n",
              "      <td>0.733608</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.306133</td>\n",
              "      <td>...</td>\n",
              "      <td>0.223160</td>\n",
              "      <td>0.588722</td>\n",
              "      <td>0.794375</td>\n",
              "      <td>0.309188</td>\n",
              "      <td>0.653845</td>\n",
              "      <td>0.080839</td>\n",
              "      <td>0.570539</td>\n",
              "      <td>0.996229</td>\n",
              "      <td>0.304604</td>\n",
              "      <td>0.230563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-02</th>\n",
              "      <td>0.529346</td>\n",
              "      <td>0.547792</td>\n",
              "      <td>0.664913</td>\n",
              "      <td>0.908108</td>\n",
              "      <td>0.271549</td>\n",
              "      <td>0.372642</td>\n",
              "      <td>0.373394</td>\n",
              "      <td>0.604143</td>\n",
              "      <td>0.998743</td>\n",
              "      <td>0.080364</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124826</td>\n",
              "      <td>0.566352</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.241899</td>\n",
              "      <td>0.599699</td>\n",
              "      <td>0.281819</td>\n",
              "      <td>0.552243</td>\n",
              "      <td>0.623507</td>\n",
              "      <td>0.248502</td>\n",
              "      <td>0.511228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>0.349326</td>\n",
              "      <td>0.071269</td>\n",
              "      <td>0.550437</td>\n",
              "      <td>0.908677</td>\n",
              "      <td>0.099480</td>\n",
              "      <td>0.745250</td>\n",
              "      <td>0.271893</td>\n",
              "      <td>0.626964</td>\n",
              "      <td>0.910748</td>\n",
              "      <td>0.253894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499382</td>\n",
              "      <td>0.647539</td>\n",
              "      <td>0.707500</td>\n",
              "      <td>0.345578</td>\n",
              "      <td>0.574179</td>\n",
              "      <td>0.410663</td>\n",
              "      <td>0.514682</td>\n",
              "      <td>0.878693</td>\n",
              "      <td>0.212153</td>\n",
              "      <td>0.361810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>0.468801</td>\n",
              "      <td>0.136579</td>\n",
              "      <td>0.625138</td>\n",
              "      <td>0.907539</td>\n",
              "      <td>0.217833</td>\n",
              "      <td>0.564897</td>\n",
              "      <td>0.017491</td>\n",
              "      <td>0.552883</td>\n",
              "      <td>0.698931</td>\n",
              "      <td>0.162776</td>\n",
              "      <td>...</td>\n",
              "      <td>0.367352</td>\n",
              "      <td>0.529656</td>\n",
              "      <td>0.993125</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>0.589480</td>\n",
              "      <td>0.167621</td>\n",
              "      <td>0.430658</td>\n",
              "      <td>0.997486</td>\n",
              "      <td>0.151579</td>\n",
              "      <td>0.310881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>0.487656</td>\n",
              "      <td>0.182539</td>\n",
              "      <td>0.465449</td>\n",
              "      <td>0.811323</td>\n",
              "      <td>0.103705</td>\n",
              "      <td>0.786723</td>\n",
              "      <td>0.138327</td>\n",
              "      <td>0.583955</td>\n",
              "      <td>0.470145</td>\n",
              "      <td>0.261534</td>\n",
              "      <td>...</td>\n",
              "      <td>0.206896</td>\n",
              "      <td>0.359012</td>\n",
              "      <td>0.999375</td>\n",
              "      <td>0.083599</td>\n",
              "      <td>0.569789</td>\n",
              "      <td>0.343082</td>\n",
              "      <td>0.428890</td>\n",
              "      <td>0.826524</td>\n",
              "      <td>0.136759</td>\n",
              "      <td>0.143848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>0.553681</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.462616</td>\n",
              "      <td>0.510839</td>\n",
              "      <td>0.100065</td>\n",
              "      <td>0.595747</td>\n",
              "      <td>0.250929</td>\n",
              "      <td>0.545317</td>\n",
              "      <td>0.438718</td>\n",
              "      <td>0.090799</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013518</td>\n",
              "      <td>0.519140</td>\n",
              "      <td>0.873750</td>\n",
              "      <td>0.192135</td>\n",
              "      <td>0.591360</td>\n",
              "      <td>0.161005</td>\n",
              "      <td>0.514390</td>\n",
              "      <td>0.959774</td>\n",
              "      <td>0.192168</td>\n",
              "      <td>0.035049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-28</th>\n",
              "      <td>0.605186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.464586</td>\n",
              "      <td>0.578777</td>\n",
              "      <td>0.084399</td>\n",
              "      <td>0.608687</td>\n",
              "      <td>0.004531</td>\n",
              "      <td>0.553155</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.112949</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419513</td>\n",
              "      <td>0.551952</td>\n",
              "      <td>0.862500</td>\n",
              "      <td>0.176839</td>\n",
              "      <td>0.578347</td>\n",
              "      <td>0.409672</td>\n",
              "      <td>0.530884</td>\n",
              "      <td>0.954745</td>\n",
              "      <td>0.166381</td>\n",
              "      <td>0.020342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-29</th>\n",
              "      <td>0.595746</td>\n",
              "      <td>0.028346</td>\n",
              "      <td>0.550411</td>\n",
              "      <td>0.292347</td>\n",
              "      <td>0.150488</td>\n",
              "      <td>0.574579</td>\n",
              "      <td>0.025648</td>\n",
              "      <td>0.572808</td>\n",
              "      <td>0.528598</td>\n",
              "      <td>0.117963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408076</td>\n",
              "      <td>0.606053</td>\n",
              "      <td>0.888125</td>\n",
              "      <td>0.244527</td>\n",
              "      <td>0.573716</td>\n",
              "      <td>0.369496</td>\n",
              "      <td>0.587936</td>\n",
              "      <td>0.949717</td>\n",
              "      <td>0.259198</td>\n",
              "      <td>0.075689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>0.573165</td>\n",
              "      <td>0.028346</td>\n",
              "      <td>0.545643</td>\n",
              "      <td>0.401991</td>\n",
              "      <td>0.163337</td>\n",
              "      <td>0.585072</td>\n",
              "      <td>0.229898</td>\n",
              "      <td>0.552204</td>\n",
              "      <td>0.949717</td>\n",
              "      <td>0.124941</td>\n",
              "      <td>...</td>\n",
              "      <td>0.321171</td>\n",
              "      <td>0.481893</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.129050</td>\n",
              "      <td>0.547839</td>\n",
              "      <td>0.145117</td>\n",
              "      <td>0.533479</td>\n",
              "      <td>0.989943</td>\n",
              "      <td>0.172413</td>\n",
              "      <td>0.101514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-31</th>\n",
              "      <td>0.596363</td>\n",
              "      <td>0.182539</td>\n",
              "      <td>0.532268</td>\n",
              "      <td>0.871465</td>\n",
              "      <td>0.176403</td>\n",
              "      <td>0.653204</td>\n",
              "      <td>0.243778</td>\n",
              "      <td>0.617990</td>\n",
              "      <td>0.746700</td>\n",
              "      <td>0.224702</td>\n",
              "      <td>...</td>\n",
              "      <td>0.348421</td>\n",
              "      <td>0.593456</td>\n",
              "      <td>0.836250</td>\n",
              "      <td>0.244672</td>\n",
              "      <td>0.645868</td>\n",
              "      <td>0.150578</td>\n",
              "      <td>0.469133</td>\n",
              "      <td>0.883721</td>\n",
              "      <td>0.128259</td>\n",
              "      <td>0.282224</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1095 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "leadtime          1                                                 2   \\\n",
              "vars             t2m       tp        H         C         E         t2m   \n",
              "2017-01-01  0.606345  0.221450  0.681078  0.880910  0.327974  0.586197   \n",
              "2017-01-02  0.529346  0.547792  0.664913  0.908108  0.271549  0.372642   \n",
              "2017-01-03  0.349326  0.071269  0.550437  0.908677  0.099480  0.745250   \n",
              "2017-01-04  0.468801  0.136579  0.625138  0.907539  0.217833  0.564897   \n",
              "2017-01-05  0.487656  0.182539  0.465449  0.811323  0.103705  0.786723   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2019-12-27  0.553681  0.000000  0.462616  0.510839  0.100065  0.595747   \n",
              "2019-12-28  0.605186  0.000000  0.464586  0.578777  0.084399  0.608687   \n",
              "2019-12-29  0.595746  0.028346  0.550411  0.292347  0.150488  0.574579   \n",
              "2019-12-30  0.573165  0.028346  0.545643  0.401991  0.163337  0.585072   \n",
              "2019-12-31  0.596363  0.182539  0.532268  0.871465  0.176403  0.653204   \n",
              "\n",
              "leadtime                                            ...        9             \\\n",
              "vars             tp        H         C         E    ...       tp        H     \n",
              "2017-01-01  0.621545  0.733608  1.000000  0.306133  ...  0.223160  0.588722   \n",
              "2017-01-02  0.373394  0.604143  0.998743  0.080364  ...  0.124826  0.566352   \n",
              "2017-01-03  0.271893  0.626964  0.910748  0.253894  ...  0.499382  0.647539   \n",
              "2017-01-04  0.017491  0.552883  0.698931  0.162776  ...  0.367352  0.529656   \n",
              "2017-01-05  0.138327  0.583955  0.470145  0.261534  ...  0.206896  0.359012   \n",
              "...              ...       ...       ...       ...  ...       ...       ...   \n",
              "2019-12-27  0.250929  0.545317  0.438718  0.090799  ...  0.013518  0.519140   \n",
              "2019-12-28  0.004531  0.553155  0.488372  0.112949  ...  0.419513  0.551952   \n",
              "2019-12-29  0.025648  0.572808  0.528598  0.117963  ...  0.408076  0.606053   \n",
              "2019-12-30  0.229898  0.552204  0.949717  0.124941  ...  0.321171  0.481893   \n",
              "2019-12-31  0.243778  0.617990  0.746700  0.224702  ...  0.348421  0.593456   \n",
              "\n",
              "leadtime                              10                                \\\n",
              "vars             C         E         t2m       tp        H         C     \n",
              "2017-01-01  0.794375  0.309188  0.653845  0.080839  0.570539  0.996229   \n",
              "2017-01-02  0.703125  0.241899  0.599699  0.281819  0.552243  0.623507   \n",
              "2017-01-03  0.707500  0.345578  0.574179  0.410663  0.514682  0.878693   \n",
              "2017-01-04  0.993125  0.225806  0.589480  0.167621  0.430658  0.997486   \n",
              "2017-01-05  0.999375  0.083599  0.569789  0.343082  0.428890  0.826524   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2019-12-27  0.873750  0.192135  0.591360  0.161005  0.514390  0.959774   \n",
              "2019-12-28  0.862500  0.176839  0.578347  0.409672  0.530884  0.954745   \n",
              "2019-12-29  0.888125  0.244527  0.573716  0.369496  0.587936  0.949717   \n",
              "2019-12-30  0.890000  0.129050  0.547839  0.145117  0.533479  0.989943   \n",
              "2019-12-31  0.836250  0.244672  0.645868  0.150578  0.469133  0.883721   \n",
              "\n",
              "leadtime                    0   \n",
              "vars             E         tp   \n",
              "2017-01-01  0.304604  0.230563  \n",
              "2017-01-02  0.248502  0.511228  \n",
              "2017-01-03  0.212153  0.361810  \n",
              "2017-01-04  0.151579  0.310881  \n",
              "2017-01-05  0.136759  0.143848  \n",
              "...              ...       ...  \n",
              "2019-12-27  0.192168  0.035049  \n",
              "2019-12-28  0.166381  0.020342  \n",
              "2019-12-29  0.259198  0.075689  \n",
              "2019-12-30  0.172413  0.101514  \n",
              "2019-12-31  0.128259  0.282224  \n",
              "\n",
              "[1095 rows x 51 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eK4tpT9v3ale"
      },
      "outputs": [],
      "source": [
        "date_index = pd.date_range('2000-01-01','2016-12-22',freq='D') # changed to 2000-01-01 from 2000-01-10\n",
        "break_index = [0]+[list(date_index).index(pd.to_datetime('%s-12-31'%year))+1\n",
        " for year in range(2000,2016)] + [len(date_index)]\n",
        "\n",
        "def split_by_year(freq_year, break_index):\n",
        "    # freq_year = 2\n",
        "    end_index = 0\n",
        "    tscv = []\n",
        "    while True:\n",
        "        start_index = end_index\n",
        "        if start_index + freq_year >= len(break_index):\n",
        "            break\n",
        "        end_index = min(start_index+2*freq_year, len(break_index)-1)\n",
        "        tscv.append((list(range(break_index[start_index],break_index[start_index+freq_year])),\n",
        "                     list(range(break_index[start_index+freq_year],break_index[end_index]))))\n",
        "    return tscv\n",
        "\n",
        "freq_year = 2 # freq_year < total year/2  # 2000-2001 (2 years) test 2 years x4\n",
        "tscv = split_by_year(freq_year, break_index)\n",
        "\n",
        "def get_xy(series, time_step, n_feature):\n",
        "    x = series.iloc[:,:-1].T.unstack(level=0).T.values.reshape(len(series),time_step,n_feature) # time_step will be 10\n",
        "    y = pd.concat([series.iloc[:,-1].shift(-i) for i in range(time_step)], axis=1).dropna(axis=0, how='any').values\n",
        "    y = y.reshape(y.shape[0],y.shape[1],1)\n",
        "    x = x[:y.shape[0],:,:]\n",
        "    return x, y\n",
        "\n",
        "time_step, n_features = 10, 5\n",
        "train_x, train_y = get_xy(train_dataset, time_step, n_features)\n",
        "test_x, test_y = get_xy(test_dataset, time_step, n_features)\n",
        "\n",
        "input_train = Input(shape=(train_x.shape[1], train_x.shape[2]))\n",
        "output_train = Input(shape=(train_y.shape[1], train_y.shape[2]))\n",
        "\n",
        "# gridsearchCV does not take in 3D shape as inputs, only 2D\n",
        "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1]*train_x.shape[2])\n",
        "train_y = train_y.reshape(train_y.shape[0], train_y.shape[1]*train_y.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>[731, 732, 733, 734, 735, 736, 737, 738, 739, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1461, 1462, 1463, 1464, 1465, 1466, 1467, 146...</td>\n",
              "      <td>[2192, 2193, 2194, 2195, 2196, 2197, 2198, 219...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2922, 2923, 2924, 2925, 2926, 2927, 2928, 292...</td>\n",
              "      <td>[3653, 3654, 3655, 3656, 3657, 3658, 3659, 366...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[4383, 4384, 4385, 4386, 4387, 4388, 4389, 439...</td>\n",
              "      <td>[5114, 5115, 5116, 5117, 5118, 5119, 5120, 512...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  \\\n",
              "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
              "1  [1461, 1462, 1463, 1464, 1465, 1466, 1467, 146...   \n",
              "2  [2922, 2923, 2924, 2925, 2926, 2927, 2928, 292...   \n",
              "3  [4383, 4384, 4385, 4386, 4387, 4388, 4389, 439...   \n",
              "\n",
              "                                                   1  \n",
              "0  [731, 732, 733, 734, 735, 736, 737, 738, 739, ...  \n",
              "1  [2192, 2193, 2194, 2195, 2196, 2197, 2198, 219...  \n",
              "2  [3653, 3654, 3655, 3656, 3657, 3658, 3659, 366...  \n",
              "3  [5114, 5115, 5116, 5117, 5118, 5119, 5120, 512...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(tscv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6201, 50)\n",
            "(6201, 10)\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJOMDjdM3tKJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_math_ops import square\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Loss function with weights based on amplitude of y_true\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "def my_MSE_weighted2(y_true,y_pred):\n",
        "    return K.mean(tf.multiply(tf.exp(tf.multiply(2.0, y_true)), tf.square(tf.subtract(y_pred, y_true))))\n",
        "\n",
        "# scoring\n",
        "def my_custom_eval_func(y_true, y_pred):\n",
        "    # Remove 3D array warning\n",
        "    if len(y_pred.shape) == 3:\n",
        "        y_pred = y_pred.reshape(y_pred.shape[:-1])\n",
        "    return rmse(y_true, y_pred, squared=False)\n",
        "\n",
        "myenvEstimator  = make_scorer(my_custom_eval_func, greater_is_better=False)\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "class mylstm(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, n_steps=10, n_features=5,\n",
        "                 activation='relu', optimizer='adam',loss=my_MSE_weighted2,\n",
        "                 lstm=48, dense=1, verbose=1,\n",
        "                 epochs=20, batch_size=8):\n",
        "                 #learning_rate=1e-3, #weight_decay=1e-5):\n",
        "\n",
        "        # static parameters\n",
        "        self.n_steps = n_steps\n",
        "        self.n_features = n_features\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Parameters that can be optimized\n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = loss\n",
        "        self.lstm = lstm\n",
        "        self.dense = dense\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        #---------- luong attention ----------------\n",
        "        encoder_stack_h, encoder_forward_h, encoder_forward_c, encoder_backward_h, encoder_backward_c = Bidirectional(LSTM(self.lstm, activation=self.activation,\n",
        "                                                                                                                           return_state=True, return_sequences=True))(input_train)\n",
        "        encoder_last_h = concatenate([encoder_forward_h, encoder_backward_h])\n",
        "        encoder_last_c = concatenate([encoder_forward_c, encoder_backward_c])\n",
        "\n",
        "        decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n",
        "        decoder_stack_h = LSTM(self.lstm*2, activation=self.activation, return_state=False, return_sequences=True)(decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
        "        attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
        "        attention = Activation('softmax')(attention)\n",
        "        context = dot([attention, encoder_stack_h], axes=[2,1])\n",
        "        decoder_combined_context = concatenate([context, decoder_stack_h])\n",
        "        out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n",
        "        self.model = Model(inputs=input_train, outputs=out)\n",
        "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
        "\n",
        "    def fit(self, X, y, **kw):\n",
        "        X = X.reshape(X.shape[0], self.n_steps, self.n_features)\n",
        "\n",
        "        # Control display output, If parameter `verbose` is given, it will be used.\n",
        "        # If no `verbose` is given, the default value of the class is used.\n",
        "        if 'verbose' in kw.keys():\n",
        "            return self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, **kw)\n",
        "        else:\n",
        "            return self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose, **kw)\n",
        "\n",
        "\n",
        "    def predict(self, X, **kw):\n",
        "        X = X.reshape(X.shape[0], self.n_steps, self.n_features)\n",
        "\n",
        "        if 'verbose' in kw.keys():\n",
        "            return self.model.predict(X, **kw)\n",
        "        else:\n",
        "            return self.model.predict(X, verbose=self.verbose, **kw)\n",
        "\n",
        "    def score(self, X, y, **kw):\n",
        "        X = X.reshape(X.shape[0], self.n_steps, self.n_features)\n",
        "\n",
        "        # Control display output\n",
        "        if 'verbose' in kw.keys():\n",
        "            return self.model.evaluate(X, y, **kw)\n",
        "        else:\n",
        "            return self.model.evaluate(X, y, verbose=self.verbose, **kw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9r0XHLX4ARW",
        "outputId": "187d19b0-51b0-47c7-be15-9666beba9d57"
      },
      "outputs": [],
      "source": [
        "opt = Adam(learning_rate=1e-4)\n",
        "\n",
        "parameters = {'activation':('relu','tanh'), 'lstm':[32,48,64,80,100,128,144,160], 'loss':(my_MSE_weighted2, 'mse')} # Actual calculation\n",
        "\n",
        "tscv = split_by_year(freq_year, break_index)\n",
        "clf = GridSearchCV(mylstm(verbose=1, epochs=20, batch_size=8, optimizer=opt), parameters, cv=tscv, scoring=myenvEstimator)\n",
        "\n",
        "clf.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyFSh9MI4JTE"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(clf.cv_results_)\n",
        "results_df['mean_test_score'] = results_df['mean_test_score'].abs()\n",
        "results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
        "results_df = results_df.set_index(\n",
        "    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
        ").rename_axis(\"kernel\")\n",
        "results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n",
        "\n",
        "model_df = results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n",
        "# model_df.to_csv('tunings_mse2_biadam.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcjqrcMu98Se",
        "outputId": "326d5bfa-ee73-44b4-cff3-b7a3d8276ecf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([48.25649691, 35.09181929, 35.86329842, 35.25014049, 36.32160974,\n",
              "        39.80626363, 40.09244198, 38.76100546, 39.58778954, 41.10896057,\n",
              "        42.42620128, 43.13410336, 44.47147024, 44.06862843, 44.73675287,\n",
              "        43.60585213, 56.02843034, 45.176467  , 56.78962451, 56.74436426,\n",
              "        47.26439083, 57.05959105, 48.17155099, 48.85460126, 68.1179077 ,\n",
              "        59.80541694, 59.39382249, 77.48450124, 59.83895636, 68.34578526,\n",
              "        59.75040925, 59.13266689]),\n",
              " 'mean_score_time': array([0.75094962, 0.68144286, 0.72104585, 0.8548755 , 0.70880604,\n",
              "        0.78209698, 0.76868701, 0.73193061, 0.71389496, 0.75992006,\n",
              "        0.73628891, 0.76536262, 1.02915251, 0.91230369, 0.82881784,\n",
              "        0.77171475, 0.79725552, 0.93370324, 0.82907778, 0.78102207,\n",
              "        0.84719265, 0.79590333, 0.85109013, 0.80885816, 0.82633185,\n",
              "        0.74711382, 0.78590304, 0.94416595, 0.82948768, 0.88558722,\n",
              "        0.95583731, 0.76971895]),\n",
              " 'mean_test_score': array([-0.17140935, -0.16946278, -0.17191349, -0.17120344, -0.17098864,\n",
              "        -0.17155169, -0.1703691 , -0.17031134, -0.17132072, -0.17110455,\n",
              "        -0.17062835, -0.17159526, -0.17047657, -0.17168916, -0.17094317,\n",
              "        -0.17063748, -0.17124836, -0.17069685, -0.17105148, -0.17246263,\n",
              "        -0.1709784 , -0.17118051, -0.17195347, -0.17260508, -0.17077191,\n",
              "        -0.17032812, -0.1709182 , -0.17142331, -0.17104466, -0.17216353,\n",
              "        -0.1712121 , -0.17127744]),\n",
              " 'param_activation': masked_array(data=['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
              "                    'relu', 'relu', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
              "                    'tanh', 'tanh', 'tanh', 'tanh'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_loss': masked_array(data=[<function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>, 'mse',\n",
              "                    'mse', 'mse', 'mse', 'mse', 'mse', 'mse', 'mse',\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>,\n",
              "                    <function my_MSE_weighted2 at 0x7f418c210a70>, 'mse',\n",
              "                    'mse', 'mse', 'mse', 'mse', 'mse', 'mse', 'mse'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_lstm': masked_array(data=[32, 48, 64, 80, 100, 128, 144, 160, 32, 48, 64, 80,\n",
              "                    100, 128, 144, 160, 32, 48, 64, 80, 100, 128, 144, 160,\n",
              "                    32, 48, 64, 80, 100, 128, 144, 160],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 32},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 48},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 64},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 80},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 100},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 128},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 144},\n",
              "  {'activation': 'relu',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 160},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 32},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 48},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 64},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 80},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 100},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 128},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 144},\n",
              "  {'activation': 'relu', 'loss': 'mse', 'lstm': 160},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 32},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 48},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 64},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 80},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 100},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 128},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 144},\n",
              "  {'activation': 'tanh',\n",
              "   'loss': <function __main__.my_MSE_weighted2>,\n",
              "   'lstm': 160},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 32},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 48},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 64},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 80},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 100},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 128},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 144},\n",
              "  {'activation': 'tanh', 'loss': 'mse', 'lstm': 160}],\n",
              " 'rank_test_score': array([23,  1, 28, 18, 13, 25,  4,  2, 22, 16,  6, 26,  5, 27, 11,  7, 20,\n",
              "         8, 15, 31, 12, 17, 29, 32,  9,  3, 10, 24, 14, 30, 19, 21],\n",
              "       dtype=int32),\n",
              " 'split0_test_score': array([-0.15736037, -0.15465162, -0.15660644, -0.15544768, -0.15653382,\n",
              "        -0.15617891, -0.15476529, -0.15496171, -0.15781659, -0.15699332,\n",
              "        -0.15587   , -0.1563561 , -0.15702696, -0.15685038, -0.15707761,\n",
              "        -0.15568924, -0.1553528 , -0.15670911, -0.15649655, -0.1573983 ,\n",
              "        -0.15623663, -0.15628862, -0.15608992, -0.15729449, -0.15580183,\n",
              "        -0.15641691, -0.15658377, -0.15549015, -0.15637872, -0.15726112,\n",
              "        -0.15614527, -0.15654619]),\n",
              " 'split1_test_score': array([-0.16423658, -0.16134857, -0.16260877, -0.16321845, -0.16289693,\n",
              "        -0.16429865, -0.16052987, -0.16328015, -0.16289861, -0.16424274,\n",
              "        -0.16194532, -0.16360664, -0.16406043, -0.16443796, -0.16257938,\n",
              "        -0.1634551 , -0.16409266, -0.16454183, -0.16317288, -0.16328713,\n",
              "        -0.1655104 , -0.16409952, -0.16417448, -0.16460634, -0.16313489,\n",
              "        -0.16403792, -0.16283831, -0.16378156, -0.16220745, -0.16461922,\n",
              "        -0.16225763, -0.16283127]),\n",
              " 'split2_test_score': array([-0.16872236, -0.16838709, -0.16994499, -0.16894777, -0.16946546,\n",
              "        -0.1697488 , -0.16985403, -0.16993397, -0.16958602, -0.16878023,\n",
              "        -0.16919908, -0.16885879, -0.16933543, -0.16886208, -0.1696146 ,\n",
              "        -0.16846812, -0.17089331, -0.16857555, -0.16941906, -0.16892085,\n",
              "        -0.16912795, -0.16912886, -0.1693244 , -0.16867578, -0.16963548,\n",
              "        -0.16845023, -0.17002182, -0.16937045, -0.16922219, -0.16906634,\n",
              "        -0.17032055, -0.17015441]),\n",
              " 'split3_test_score': array([-0.19531809, -0.19346385, -0.19849376, -0.19719986, -0.19505835,\n",
              "        -0.19598039, -0.19632722, -0.19306954, -0.19498165, -0.19440193,\n",
              "        -0.19549902, -0.1975595 , -0.19148346, -0.19660622, -0.19450108,\n",
              "        -0.19493745, -0.19465469, -0.19296091, -0.19511744, -0.20024425,\n",
              "        -0.19303864, -0.19520503, -0.1982251 , -0.1998437 , -0.19451544,\n",
              "        -0.19240741, -0.1942289 , -0.19705109, -0.19637026, -0.19770744,\n",
              "        -0.19612497, -0.19557789]),\n",
              " 'std_fit_time': array([22.32332316,  0.62067224,  0.4616905 ,  0.50007303,  0.46581067,\n",
              "         2.98550326,  3.20891627,  0.18801934,  0.39535551,  0.75768847,\n",
              "         1.60870719,  1.29460563,  0.97882678,  0.7958489 ,  0.72995304,\n",
              "         0.55167201, 18.08219647,  0.61409182, 17.00049084, 16.92864544,\n",
              "         0.84924934, 17.11877853,  0.84501241,  1.3860608 , 18.60253505,\n",
              "        15.6629594 , 15.41745343, 14.97983233, 15.14197227, 17.70211773,\n",
              "        15.11631953, 15.3509352 ]),\n",
              " 'std_score_time': array([0.08847175, 0.01573413, 0.05365117, 0.3011439 , 0.05857107,\n",
              "        0.08917923, 0.05291128, 0.05112248, 0.03475326, 0.06033292,\n",
              "        0.03229925, 0.06167774, 0.34780441, 0.23913508, 0.0708955 ,\n",
              "        0.05315591, 0.05574858, 0.19725424, 0.10891573, 0.06110521,\n",
              "        0.08688611, 0.04969115, 0.05576621, 0.05958542, 0.09220472,\n",
              "        0.03815095, 0.01959336, 0.25631048, 0.06009965, 0.03369584,\n",
              "        0.30102931, 0.03328061]),\n",
              " 'std_test_score': array([0.01438463, 0.01468349, 0.01605669, 0.0157552 , 0.0146295 ,\n",
              "        0.01490755, 0.01592466, 0.01416974, 0.0142841 , 0.01409239,\n",
              "        0.01511454, 0.01563394, 0.01289041, 0.0150135 , 0.01430862,\n",
              "        0.01474984, 0.01459328, 0.01354372, 0.01462663, 0.01654907,\n",
              "        0.01357661, 0.01460561, 0.01588455, 0.01624635, 0.0145557 ,\n",
              "        0.01345465, 0.01427369, 0.01559855, 0.01531252, 0.01533853,\n",
              "        0.01523678, 0.01483341])}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.cv_results_"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
